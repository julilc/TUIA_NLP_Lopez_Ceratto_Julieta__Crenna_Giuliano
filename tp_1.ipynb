{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Trabajo Práctico N°1\n",
    "**Integrantes**\n",
    "- López Ceratto, Julieta : L-3311/1\n",
    "- Crenna, Giuliano : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\julil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rqst import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_juegos = pd.read_csv('./src/bgg_database.csv')\n",
    "dataset_peliculas = pd.read_csv('./src/IMDB-Movie-Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gutenberg.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ruta_url = 'https://www.gutenberg.org/browse/scores/top1000.php#books-last1'\n",
    "response = requests.get(ruta_url, verify=False)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "ol = soup.findAll('ol')\n",
    "libros = []\n",
    "for libro in ol:\n",
    "    enlaces = libro.findAll('li')\n",
    "    for enlace in enlaces:\n",
    "        libros.append(enlace.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = [libro for libro in libros if libro != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expresión regular para extraer las partes\n",
    "patron_titulo_principal = r'^(.*?)(?:;?\\s?Or,|\\sby|\\s\\()'  # Captura todo hasta \"Or,\" o \"by\" o \"(\" si no hay más\n",
    "patron_titulo_secundario = r';?\\s?Or,?\\s(.*?)\\sby'  # Captura el título secundario si está\n",
    "patron_autor = r'by\\s(.*?)\\s\\('  # Captura el autor si está\n",
    "patron_n_ref = r'\\((\\d+)\\)'  # Captura el número entre paréntesis\n",
    "patron_link = r'\\)\\s(.*)'  # Captura todo después del paréntesis final para el enlace\n",
    "titulo_principal = []\n",
    "titulo_secundario = []\n",
    "autor = []\n",
    "n_ref = []\n",
    "for libro in libros:\n",
    "    # Extraer título principal\n",
    "    titulo = re.search(patron_titulo_principal, libro)\n",
    "    titulo_principal.append(titulo.group(1) if titulo else \"\")\n",
    "\n",
    "    # Extraer título secundario (si existe)\n",
    "    secundario = re.search(patron_titulo_secundario, libro)\n",
    "    titulo_secundario.append(secundario.group(1) if secundario else \"\")\n",
    "\n",
    "    # Extraer autor (si existe)\n",
    "    autor_libro = re.search(patron_autor, libro)\n",
    "    autor.append(autor_libro.group(1) if autor_libro else \"\")\n",
    "\n",
    "    # Extraer número de referencia\n",
    "    ref = re.search(patron_n_ref, libro)\n",
    "    n_ref.append(ref.group(1) if ref else \"\")\n",
    "dataset_libros = pd.DataFrame({\n",
    "    'Titulo Principal': titulo_principal,\n",
    "    'Titulo Secundario': titulo_secundario,\n",
    "    'Autor': autor,\n",
    "    'N° Ref': n_ref,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_libros.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_libros.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos el dataset como csv para evitar volver a hacer el web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_libros.to_csv('./src/dataset_libros.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos Dataset Libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_libros = pd.read_csv('./src/dataset_libros.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducciones de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se analiza la presencia de null, distribuciones de los datasets, balanceo, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Juegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Libros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
